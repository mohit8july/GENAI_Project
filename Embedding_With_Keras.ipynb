{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NElpAQX1u--N"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line imports the Sequential class from the keras.models module. Sequential is a linear stack of layers which allows you to create models layer-by-layer in a step-by-step fashion"
      ],
      "metadata": {
        "id": "rzOLuD-1vdH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "zolWgQ8cvbd_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the Embedding layer is imported from the keras.layers module. An Embedding layer is used in neural networks to create word embeddings, which are dense vector representations of words. This can be used for text inputs where each word is represented by an integer index."
      ],
      "metadata": {
        "id": "zWq_bPjHvhT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Sequential model\n",
        "model = Sequential()"
      ],
      "metadata": {
        "id": "9TeMrR5EvfWX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An instance of the Sequential class is created and assigned to the variable model. This instance will be used to stack layers, starting with an embedding layer"
      ],
      "metadata": {
        "id": "-KO0K4t7vlgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an Embedding layer expecting input vocab of size 10, and output embedding dimension of size 50.\n",
        "model.add(Embedding(input_dim=10, output_dim=50))"
      ],
      "metadata": {
        "id": "RSww-mKRvjs7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Embedding layer is added to the model. The input_dim parameter is set to 10, indicating the size of the vocabulary (the total number of unique words). The output_dim parameter is set to 50, meaning each word index will be mapped to a 50-dimensional vector. The model will learn these embeddings during training"
      ],
      "metadata": {
        "id": "CG5mYRbDvqU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model will output a 2D vector with one embedding for each word in the input sequence of words (here, we assume length 1).\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODQGNIHCvoTW",
        "outputId": "db1d669e-6a65-4a17-acd7-7cb7e9879b99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          500       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 500 (1.95 KB)\n",
            "Trainable params: 500 (1.95 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, model.summary() prints a summary representation of the model.\n",
        "\n",
        "For a real input, you would need to specify the input_length parameter in the Embedding layer if your input sequences are of fixed size"
      ],
      "metadata": {
        "id": "PYu_1ISJvunr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcbviLWIvsgF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}